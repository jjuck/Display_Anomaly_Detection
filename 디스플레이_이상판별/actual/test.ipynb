{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Normal: 38156, AbNormal: 2350\n",
      "F1 Score: 0.6694\n",
      "Recall: 0.7940\n",
      "Accuracy: 0.5950\n",
      "Precision: 0.5786\n",
      "The ratio of abnormal is: 0.23362709521340935\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))\n",
    "\n",
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = train_data[['Receip No Collect Result_Dam', '1st Pressure 1st Pressure Unit Time_AutoClave', '3rd Pressure Collect Result_AutoClave', 'Chamber Temp. Unit Time_AutoClave', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2',\n",
    "                         'Head Clean Position Z Collect Result_Dam',\n",
    "                         'target']]\n",
    "\n",
    "# 'Head Clean Position Z Collect Result_Dam' 컬럼을 120 이상인 값과 이하인 값으로 이진분류\n",
    "train_data['Head Clean Position Z Collect Result_Dam'] = train_data['Head Clean Position Z Collect Result_Dam'] >= 120\n",
    "\n",
    "# 이진분류 결과가 True/False로 나올 텐데, 이를 1과 0으로 바꾸려면 다음과 같이 할 수 있습니다.\n",
    "train_data['Head Clean Position Z Collect Result_Dam'] = train_data['Head Clean Position Z Collect Result_Dam'].astype(int)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")\n",
    "\n",
    "# Split the data into features and target\n",
    "train_x = df_concat.drop(columns=['target'])\n",
    "train_y = df_concat['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Recursive Feature Elimination (RFE) for backward feature selection\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=400,\n",
    "    l2_leaf_reg=5,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_rfe = test_data[X_train.columns]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_rfe)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['Wip Line_Dam', 'Process Desc._Dam', 'Equipment_Dam',\n",
      "       'Model.Suffix_Dam', 'Workorder_Dam', 'Insp Judge Code_Dam',\n",
      "       'Wip Line_AutoClave', 'Process Desc._AutoClave', 'Equipment_AutoClave',\n",
      "       'Model.Suffix_AutoClave', 'Workorder_AutoClave',\n",
      "       'Insp Judge Code_AutoClave', '1st Pressure Judge Value_AutoClave',\n",
      "       '2nd Pressure Judge Value_AutoClave',\n",
      "       '3rd Pressure Judge Value_AutoClave',\n",
      "       'Chamber Temp. Judge Value_AutoClave', 'Wip Line_Fill1',\n",
      "       'Process Desc._Fill1', 'Equipment_Fill1', 'Model.Suffix_Fill1',\n",
      "       'Workorder_Fill1', 'Insp Judge Code_Fill1', 'Wip Line_Fill2',\n",
      "       'Process Desc._Fill2', 'Equipment_Fill2', 'Model.Suffix_Fill2',\n",
      "       'Workorder_Fill2', 'Insp Judge Code_Fill2', 'target'],\n",
      "      dtype='object')\n",
      "Total: Normal: 38156, AbNormal: 2350\n",
      "F1 Score: 0.6686\n",
      "Recall: 0.8008\n",
      "Accuracy: 0.5901\n",
      "Precision: 0.5738\n",
      "Non-numeric columns: Index(['Set ID', 'Wip Line_Dam', 'Process Desc._Dam', 'Equipment_Dam',\n",
      "       'Model.Suffix_Dam', 'Workorder_Dam', 'Insp Judge Code_Dam',\n",
      "       'Wip Line_AutoClave', 'Process Desc._AutoClave', 'Equipment_AutoClave',\n",
      "       'Model.Suffix_AutoClave', 'Workorder_AutoClave',\n",
      "       'Insp Judge Code_AutoClave', '1st Pressure Judge Value_AutoClave',\n",
      "       '2nd Pressure Judge Value_AutoClave',\n",
      "       '3rd Pressure Judge Value_AutoClave',\n",
      "       'Chamber Temp. Judge Value_AutoClave', 'Wip Line_Fill1',\n",
      "       'Process Desc._Fill1', 'Equipment_Fill1', 'Model.Suffix_Fill1',\n",
      "       'Workorder_Fill1', 'Insp Judge Code_Fill1', 'Wip Line_Fill2',\n",
      "       'Process Desc._Fill2', 'Equipment_Fill2', 'Model.Suffix_Fill2',\n",
      "       'Workorder_Fill2', 'Insp Judge Code_Fill2'],\n",
      "      dtype='object')\n",
      "The ratio of abnormal is: 0.22441103623063188\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))\n",
    "\n",
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = train_data[['Receip No Collect Result_Dam', '1st Pressure 1st Pressure Unit Time_AutoClave', '3rd Pressure Collect Result_AutoClave', 'Chamber Temp. Unit Time_AutoClave', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2', 'target']]\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")\n",
    "\n",
    "# Split the data into features and target\n",
    "train_x = df_concat.drop(columns=['target'])\n",
    "train_y = df_concat['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# CatBoost classifier for SFS\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=400,\n",
    "    l2_leaf_reg=5,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Sequential Feature Selector (SFS)\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select=\"auto\",  # Auto will select a feature set with the best score\n",
    "    direction='forward',  # Can be 'forward' or 'backward'\n",
    "    scoring='f1',  # You can use 'f1', 'precision', 'recall', etc.\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get selected feature names and apply to both training and test data\n",
    "selected_features = X_train.columns[sfs.get_support()]\n",
    "X_train_sfs = X_train[selected_features]\n",
    "X_test_sfs = X_test[selected_features]\n",
    "\n",
    "# Train final model on selected features\n",
    "model.fit(X_train_sfs, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_sfs)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_sfs = test_data[selected_features]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_sfs)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))\n",
    "\n",
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = train_data[['Receip No Collect Result_Dam', '1st Pressure 1st Pressure Unit Time_AutoClave', '3rd Pressure Collect Result_AutoClave', 'Chamber Temp. Unit Time_AutoClave', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2', 'target']]\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")\n",
    "\n",
    "# Split the data into features and target\n",
    "train_x = df_concat.drop(columns=['target'])\n",
    "train_y = df_concat['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# CatBoost classifier for RFE\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=400,\n",
    "    l2_leaf_reg=5,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(\n",
    "    estimator=model,\n",
    "    n_features_to_select=5,  # Adjust based on how many features you want to select\n",
    "    step=1  # Number of features to remove at each iteration\n",
    ")\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features and apply to both training and test data\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "X_train_rfe = X_train[selected_features]\n",
    "X_test_rfe = X_test[selected_features]\n",
    "\n",
    "# Train final model on selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_rfe)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_rfe = test_data[selected_features]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_rfe)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Normal: 38156, AbNormal: 2350\n",
      "F1 Score: 0.6620\n",
      "Recall: 0.7775\n",
      "Accuracy: 0.5901\n",
      "Precision: 0.5764\n",
      "The ratio of abnormal is: 0.2429007545648292\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))\n",
    "\n",
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def generating_features(df):\n",
    "    # Feature engineering for thickness and pressure\n",
    "    df['Thickness_Diff_1_2'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 2 Collect Result_Dam']\n",
    "    df['Thickness_Diff_2_3'] = df['THICKNESS 2 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "    df['Thickness_Diff_1_3'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "\n",
    "    df['Thickness_Avg'] = (\n",
    "        df['THICKNESS 1 Collect Result_Dam'] +\n",
    "        df['THICKNESS 2 Collect Result_Dam'] +\n",
    "        df['THICKNESS 3 Collect Result_Dam']) / 3\n",
    "\n",
    "    df['Thickness_Std'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].std(axis=1)\n",
    "\n",
    "    df['Pressure_Diff_1st_2nd'] = df['1st Pressure Collect Result_AutoClave'] - df['2nd Pressure Collect Result_AutoClave']\n",
    "    df['Pressure_Diff_2nd_3rd'] = df['2nd Pressure Collect Result_AutoClave'] - df['3rd Pressure Collect Result_AutoClave']\n",
    "    df['Pressure_Diff_1st_3rd'] = df['1st Pressure Collect Result_AutoClave'] - df['3rd Pressure Collect Result_AutoClave']\n",
    "\n",
    "    df['Pressure_Avg'] = (\n",
    "        df['1st Pressure Collect Result_AutoClave'] +\n",
    "        df['2nd Pressure Collect Result_AutoClave'] +\n",
    "        df['3rd Pressure Collect Result_AutoClave']) / 3\n",
    "\n",
    "    df['Pressure_Std'] = df[['1st Pressure Collect Result_AutoClave', '2nd Pressure Collect Result_AutoClave', '3rd Pressure Collect Result_AutoClave']].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = generating_features(train_data)\n",
    "train_data = train_data[['Receip No Collect Result_Dam', '1st Pressure 1st Pressure Unit Time_AutoClave', '3rd Pressure Collect Result_AutoClave', 'Chamber Temp. Unit Time_AutoClave', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2', \n",
    "                         'Thickness_Diff_1_2', 'Thickness_Diff_2_3', 'Thickness_Diff_1_3', 'Thickness_Avg', 'Thickness_Std', 'Pressure_Diff_1st_2nd', 'Pressure_Diff_2nd_3rd', 'Pressure_Diff_1st_3rd', 'Pressure_Avg', 'Pressure_Std',\n",
    "                         'target']]\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")\n",
    "\n",
    "# Split the data into features and target\n",
    "train_x = df_concat.drop(columns=['target'])\n",
    "train_y = df_concat['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# CatBoost classifier for SFS\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=400,\n",
    "    l2_leaf_reg=5,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Sequential Feature Selector (SFS)\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select=\"auto\",  # Auto will select a feature set with the best score\n",
    "    direction='forward',  # Can be 'forward' or 'backward'\n",
    "    scoring='f1',  # You can use 'f1', 'precision', 'recall', etc.\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1  # Use all available CPU cores\n",
    ")\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get selected feature names and apply to both training and test data\n",
    "selected_features = X_train.columns[sfs.get_support()]\n",
    "X_train_sfs = X_train[selected_features]\n",
    "X_test_sfs = X_test[selected_features]\n",
    "\n",
    "# Train final model on selected features\n",
    "model.fit(X_train_sfs, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test_sfs)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "test_data = generating_features(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_sfs = test_data[selected_features]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_sfs)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same_rows_columns ['Wip Line_Dam', 'Process Desc._Dam', 'Insp. Seq No._Dam', 'Insp Judge Code_Dam', 'CURE STANDBY POSITION X Collect Result_Dam', 'CURE STANDBY POSITION Z Collect Result_Dam', 'CURE STANDBY POSITION Θ Collect Result_Dam', 'CURE START POSITION Z Collect Result_Dam', 'HEAD Standby Position X Collect Result_Dam', 'HEAD Standby Position Y Collect Result_Dam', 'HEAD Standby Position Z Collect Result_Dam', 'Head Clean Position X Collect Result_Dam', 'Head Clean Position Y Collect Result_Dam', 'Head Purge Position X Collect Result_Dam', 'Head Purge Position Y Collect Result_Dam', 'Head Zero Position X Collect Result_Dam', 'WorkMode Collect Result_Dam', 'Wip Line_AutoClave', 'Process Desc._AutoClave', 'Equipment_AutoClave', 'Insp. Seq No._AutoClave', 'Insp Judge Code_AutoClave', '1st Pressure Judge Value_AutoClave', '2nd Pressure Judge Value_AutoClave', '3rd Pressure Judge Value_AutoClave', 'Wip Line_Fill1', 'Process Desc._Fill1', 'Insp. Seq No._Fill1', 'Insp Judge Code_Fill1', 'HEAD Standby Position X Collect Result_Fill1', 'HEAD Standby Position Y Collect Result_Fill1', 'HEAD Standby Position Z Collect Result_Fill1', 'Head Clean Position X Collect Result_Fill1', 'Head Clean Position Y Collect Result_Fill1', 'Head Clean Position Z Collect Result_Fill1', 'Head Purge Position X Collect Result_Fill1', 'Head Purge Position Y Collect Result_Fill1', 'WorkMode Collect Result_Fill1', 'Wip Line_Fill2', 'Process Desc._Fill2', 'Insp. Seq No._Fill2', 'Insp Judge Code_Fill2', 'CURE END POSITION Θ Collect Result_Fill2', 'CURE STANDBY POSITION X Collect Result_Fill2', 'CURE STANDBY POSITION Θ Collect Result_Fill2', 'CURE START POSITION Θ Collect Result_Fill2', 'DISCHARGED SPEED OF RESIN Collect Result_Fill2', 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2', 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2', 'Dispense Volume(Stage1) Collect Result_Fill2', 'Dispense Volume(Stage2) Collect Result_Fill2', 'Dispense Volume(Stage3) Collect Result_Fill2', 'HEAD Standby Position X Collect Result_Fill2', 'HEAD Standby Position Y Collect Result_Fill2', 'HEAD Standby Position Z Collect Result_Fill2', 'Head Clean Position X Collect Result_Fill2', 'Head Clean Position Y Collect Result_Fill2', 'Head Clean Position Z Collect Result_Fill2', 'Head Purge Position X Collect Result_Fill2', 'Head Purge Position Y Collect Result_Fill2', 'WorkMode Collect Result_Fill2']\n",
      "matching_row_columns []\n",
      "Non-numeric columns: Index(['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',\n",
      "       'Model.Suffix_AutoClave', 'Workorder_AutoClave',\n",
      "       'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',\n",
      "       'Model.Suffix_Fill1', 'Workorder_Fill1', 'Equipment_Fill2',\n",
      "       'Model.Suffix_Fill2', 'Workorder_Fill2', 'target'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HEAD Standby Position X Collect Result_Dam'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HEAD Standby Position X Collect Result_Dam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m train_data \u001b[38;5;241m=\u001b[39m cat2num(train_data)\n\u001b[0;32m    134\u001b[0m train_data \u001b[38;5;241m=\u001b[39m generating_features(train_data)\n\u001b[1;32m--> 135\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mcart2sph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m    138\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "Cell \u001b[1;32mIn[4], line 94\u001b[0m, in \u001b[0;36mcart2sph\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     84\u001b[0m cylindrical_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE START POSITION X Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE START POSITION Z Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE START POSITION Θ Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     86\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE STANDBY POSITION X Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE STANDBY POSITION Z Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE STANDBY POSITION Θ Collect Result_Dam\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE END POSITION X Collect Result_Fill2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE END POSITION Z Collect Result_Fill2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCURE END POSITION Θ Collect Result_Fill2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     91\u001b[0m ]\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_col, y_col, z_col \u001b[38;5;129;01min\u001b[39;00m coordinate_columns:\n\u001b[1;32m---> 94\u001b[0m     df[x_col] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m df[y_col]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m df[z_col]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     95\u001b[0m     df[y_col] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marctan2(df[y_col], df[x_col])\n\u001b[0;32m     96\u001b[0m     df[z_col] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marccos(df[z_col] \u001b[38;5;241m/\u001b[39m df[x_col])\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'HEAD Standby Position X Collect Result_Dam'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))\n",
    "\n",
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def generating_features(df):\n",
    "    # Feature engineering for thickness and pressure\n",
    "    df['Thickness_Diff_1_2'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 2 Collect Result_Dam']\n",
    "    df['Thickness_Diff_2_3'] = df['THICKNESS 2 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "    df['Thickness_Diff_1_3'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "\n",
    "    df['Thickness_Avg'] = (\n",
    "        df['THICKNESS 1 Collect Result_Dam'] +\n",
    "        df['THICKNESS 2 Collect Result_Dam'] +\n",
    "        df['THICKNESS 3 Collect Result_Dam']) / 3\n",
    "\n",
    "    df['Thickness_Std'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].std(axis=1)\n",
    "\n",
    "    df['Pressure_Diff_1st_2nd'] = df['1st Pressure Collect Result_AutoClave'] - df['2nd Pressure Collect Result_AutoClave']\n",
    "    df['Pressure_Diff_2nd_3rd'] = df['2nd Pressure Collect Result_AutoClave'] - df['3rd Pressure Collect Result_AutoClave']\n",
    "    df['Pressure_Diff_1st_3rd'] = df['1st Pressure Collect Result_AutoClave'] - df['3rd Pressure Collect Result_AutoClave']\n",
    "\n",
    "    df['Pressure_Avg'] = (\n",
    "        df['1st Pressure Collect Result_AutoClave'] +\n",
    "        df['2nd Pressure Collect Result_AutoClave'] +\n",
    "        df['3rd Pressure Collect Result_AutoClave']) / 3\n",
    "\n",
    "    df['Pressure_Std'] = df[['1st Pressure Collect Result_AutoClave', '2nd Pressure Collect Result_AutoClave', '3rd Pressure Collect Result_AutoClave']].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def cart2sph(df):\n",
    "    coordinate_columns = [\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam\", \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam\"),\n",
    "        (\"HEAD Standby Position X Collect Result_Dam\", \"HEAD Standby Position Y Collect Result_Dam\", \"HEAD Standby Position Z Collect Result_Dam\"),\n",
    "        (\"Head Clean Position X Collect Result_Dam\", \"Head Clean Position Y Collect Result_Dam\", \"Head Clean Position Z Collect Result_Dam\"),\n",
    "        (\"Head Purge Position X Collect Result_Dam\", \"Head Purge Position Y Collect Result_Dam\", \"Head Purge Position Z Collect Result_Dam\"),\n",
    "        (\"Head Zero Position X Collect Result_Dam\", \"Head Zero Position Y Collect Result_Dam\", \"Head Zero Position Z Collect Result_Dam\"),\n",
    "\n",
    "        (\"HEAD Standby Position X Collect Result_Fill1\", \"HEAD Standby Position Y Collect Result_Fill1\", \"HEAD Standby Position Z Collect Result_Fill1\"),\n",
    "        (\"Head Clean Position X Collect Result_Fill1\", \"Head Clean Position Y Collect Result_Fill1\", \"Head Clean Position Z Collect Result_Fill1\"),\n",
    "        (\"Head Purge Position X Collect Result_Fill1\", \"Head Purge Position Y Collect Result_Fill1\", \"Head Purge Position Z Collect Result_Fill1\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1\", \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1\"),\n",
    "\n",
    "        (\"HEAD Standby Position X Collect Result_Fill2\", \"HEAD Standby Position Y Collect Result_Fill2\", \"HEAD Standby Position Z Collect Result_Fill2\"),\n",
    "        (\"Head Clean Position X Collect Result_Fill2\", \"Head Clean Position Y Collect Result_Fill2\", \"Head Clean Position Z Collect Result_Fill2\"),\n",
    "        (\"Head Purge Position X Collect Result_Fill2\", \"Head Purge Position Y Collect Result_Fill2\", \"Head Purge Position Z Collect Result_Fill2\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2\"),\n",
    "        (\"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2\", \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2\"),\n",
    "    ]\n",
    "\n",
    "    cylindrical_columns = [\n",
    "        (\"CURE START POSITION X Collect Result_Dam\", \"CURE START POSITION Z Collect Result_Dam\", \"CURE START POSITION Θ Collect Result_Dam\"),\n",
    "        (\"CURE STANDBY POSITION X Collect Result_Dam\", \"CURE STANDBY POSITION Z Collect Result_Dam\", \"CURE STANDBY POSITION Θ Collect Result_Dam\"),\n",
    "        (\"CURE END POSITION X Collect Result_Dam\", \"CURE END POSITION Z Collect Result_Dam\", \"CURE END POSITION Θ Collect Result_Dam\"),\n",
    "        (\"CURE START POSITION X Collect Result_Fill2\", \"CURE START POSITION Z Collect Result_Fill2\", \"CURE START POSITION Θ Collect Result_Fill2\"),\n",
    "        (\"CURE STANDBY POSITION X Collect Result_Fill2\", \"CURE STANDBY POSITION Z Collect Result_Fill2\", \"CURE STANDBY POSITION Θ Collect Result_Fill2\"),\n",
    "        (\"CURE END POSITION X Collect Result_Fill2\", \"CURE END POSITION Z Collect Result_Fill2\", \"CURE END POSITION Θ Collect Result_Fill2\"),\n",
    "    ]\n",
    "\n",
    "    for x_col, y_col, z_col in coordinate_columns:\n",
    "        df[x_col] = np.sqrt(df[x_col]**2 + df[y_col]**2 + df[z_col]**2)\n",
    "        df[y_col] = np.arctan2(df[y_col], df[x_col])\n",
    "        df[z_col] = np.arccos(df[z_col] / df[x_col])\n",
    "\n",
    "        r_col = x_col.replace(\"X\", \"r\").replace(\"Y\", \"θ\").replace(\"Z\", \"φ\")\n",
    "        theta_col = y_col.replace(\"X\", \"r\").replace(\"Y\", \"θ\").replace(\"Z\", \"φ\")\n",
    "        phi_col = z_col.replace(\"X\", \"r\").replace(\"Y\", \"θ\").replace(\"Z\", \"φ\")\n",
    "\n",
    "        df.rename(columns={x_col: r_col, y_col: theta_col, z_col: phi_col}, inplace=True)\n",
    "\n",
    "    for x_col, z_col, theta_col in cylindrical_columns:\n",
    "        df[x_col] = np.sqrt(df[x_col]**2 + df[z_col]**2)\n",
    "        df[z_col] = np.arctan2(df[z_col], df[x_col])\n",
    "\n",
    "        r_col = x_col.replace(\"X\", \"r\").replace(\"Z\", \"z\").replace(\"Θ\", \"θ\")\n",
    "        df.rename(columns={x_col: r_col, z_col: theta_col}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    # Remove columns where every value is unique\n",
    "    same_rows_columns = [column for column in df.columns if df[column].nunique() == 1]\n",
    "    row_count = len(df)\n",
    "    matching_row_columns = [column for column in df.columns if df[column].value_counts().size == row_count]\n",
    "\n",
    "    # columns_to_remove = ['Wip Line_Dam', 'Process Desc._Dam', 'Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',\n",
    "    #                      'Wip Line_AutoClave', 'Process Desc._AutoClave', 'Equipment_AutoClave', 'Model.Suffix_AutoClave', 'Workorder_AutoClave',\n",
    "    #                      'Wip Line_Fill1', 'Process Desc._Fill1', 'Equipment_Fill1', 'Model.Suffix_Fill1', 'Workorder_Fill1',\n",
    "    #                      'Wip Line_Fill2', 'Process Desc._Fill2', 'Equipment_Fill2', 'Model.Suffix_Fill2', 'Workorder_Fill2']\n",
    "    columns_to_remove = []\n",
    "\n",
    "    all_columns_to_remove = list(set(same_rows_columns + matching_row_columns + columns_to_remove))\n",
    "    df.drop(columns=[col for col in all_columns_to_remove if col in df.columns], inplace=True)\n",
    "\n",
    "    print(\"same_rows_columns\", same_rows_columns)\n",
    "    print(\"matching_row_columns\", matching_row_columns)\n",
    "    return df\n",
    "\n",
    "train_data = preprocess(train_data)\n",
    "train_data = cat2num(train_data)\n",
    "train_data = generating_features(train_data)\n",
    "train_data = cart2sph(train_data)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")\n",
    "\n",
    "# Split the data into features and target\n",
    "train_x = df_concat.drop(columns=['target'])\n",
    "train_y = df_concat['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Recursive Feature Elimination (RFE) for backward feature selection\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=400,\n",
    "    l2_leaf_reg=5,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = preprocess(test_data)\n",
    "test_data = cat2num(test_data)\n",
    "test_data = generating_features(test_data)\n",
    "test_data = cart2sph(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_rfe = test_data[X_train.columns]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_rfe)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
