{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Reading data\\Dam dispensing.csv\n",
      "  Reading data\\Auto clave.csv\n",
      "  Reading data\\Fill1 dispensing.csv\n",
      "  Reading data\\Fill2 dispensing.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# 데이터 불러오기\n",
    "ROOT_DIR = \"data\"\n",
    "\n",
    "def read_excel_file(file_path: str, header: int = None) -> pd.DataFrame:\n",
    "    csv_file = file_path.replace(\".xlsx\", \".csv\")\n",
    "\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(\"Converting excel to csv...\")\n",
    "        if header:\n",
    "            df = pd.read_excel(file_path, header=header)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"  {file_path} -> {csv_file}\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"  Reading {csv_file}\")\n",
    "        return pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "X_Dam = read_excel_file(os.path.join(ROOT_DIR, \"Dam dispensing.xlsx\"), header=1)\n",
    "X_AutoClave = read_excel_file(os.path.join(ROOT_DIR, \"Auto clave.xlsx\"), header=1)\n",
    "X_Fill1 = read_excel_file(os.path.join(ROOT_DIR, \"Fill1 dispensing.xlsx\"), header=1)\n",
    "X_Fill2 = read_excel_file(os.path.join(ROOT_DIR, \"Fill2 dispensing.xlsx\"), header=1)\n",
    "y = pd.read_csv(os.path.join(ROOT_DIR, \"train_y.csv\"))\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "X_Dam.columns = [i + \" - Dam\" for i in X_Dam.columns]\n",
    "X_AutoClave.columns = [i + \" - AutoClave\" for i in X_AutoClave.columns]\n",
    "X_Fill1.columns = [i + \" - Fill1\" for i in X_Fill1.columns]\n",
    "X_Fill2.columns = [i + \" - Fill2\" for i in X_Fill2.columns]\n",
    "X_Dam = X_Dam.rename(columns={\"Set ID - Dam\": \"Set ID\"})\n",
    "X_AutoClave = X_AutoClave.rename(columns={\"Set ID - AutoClave\": \"Set ID\"})\n",
    "X_Fill1 = X_Fill1.rename(columns={\"Set ID - Fill1\": \"Set ID\"})\n",
    "X_Fill2 = X_Fill2.rename(columns={\"Set ID - Fill2\": \"Set ID\"})\n",
    "\n",
    "# 데이터 병합\n",
    "X = pd.merge(X_Dam, X_AutoClave, on=\"Set ID\")\n",
    "X = pd.merge(X, X_Fill1, on=\"Set ID\")\n",
    "X = pd.merge(X, X_Fill2, on=\"Set ID\")\n",
    "X = X.drop(X[X.duplicated(subset=\"Set ID\")].index).reset_index(drop=True)\n",
    "\n",
    "# 날짜 컬럼 제거\n",
    "date_columns = [col for col in X.columns if 'Date' in col]\n",
    "X = X.drop(columns=(date_columns))\n",
    "\n",
    "# ID 분리\n",
    "set_id = X[['Set ID']]\n",
    "\n",
    "# 비수치형 컬럼 인코딩\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "encoder = LabelEncoder()\n",
    "encoded_df = pd.DataFrame(index=X.index)\n",
    "for column in non_numeric_columns:\n",
    "    encoded_df[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "X = X.drop(columns=non_numeric_columns)\n",
    "X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_columns = X.columns\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=X_columns)\n",
    "\n",
    "# Set ID 추가\n",
    "X = X.drop(columns=['Set ID'])\n",
    "X = pd.concat([X, set_id], axis=1)\n",
    "\n",
    "# X와 y 병합\n",
    "df_merged = pd.merge(X, y, \"inner\", on=\"Set ID\")\n",
    "\n",
    "# 결측치가 절반 이상인 컬럼 제거\n",
    "drop_cols = [column for column in df_merged.columns if (df_merged[column].notnull().sum() // 2) < df_merged[column].isnull().sum()]\n",
    "df_merged = df_merged.drop(drop_cols, axis=1)\n",
    "\n",
    "# Lot ID 컬럼 제거\n",
    "df_merged = df_merged.drop(\"LOT ID - Dam\", axis=1)\n",
    "\n",
    "# 데이터 분리\n",
    "normal_ratio = 1.0  # 1:1 비율\n",
    "df_normal = df_merged[df_merged[\"target\"] == \"Normal\"]\n",
    "df_abnormal = df_merged[df_merged[\"target\"] == \"AbNormal\"]\n",
    "\n",
    "num_abnormal = len(df_abnormal)\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_train, df_val = train_test_split(df_concat, test_size=0.3, stratify=df_concat[\"target\"], random_state=RANDOM_STATE)\n",
    "\n",
    "# 피처 목록 생성\n",
    "features = [col for col in df_train.columns if col not in ['Set ID', 'target']]\n",
    "\n",
    "train_x = df_train[features].values\n",
    "train_y = (df_train[\"target\"] == \"AbNormal\").astype(int).values\n",
    "test_x = df_val[features].values\n",
    "test_y = (df_val[\"target\"] == \"AbNormal\").astype(int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Insp. Seq No. - Dam', 'Collect Result.3 - Dam', 'Collect Result.4 - Dam', 'Collect Result.5 - Dam', 'Collect Result.10 - Dam', 'Collect Result.13 - Dam', 'Collect Result.19 - Dam', 'Collect Result.31 - Dam', 'Collect Result.32 - Dam', 'Collect Result.34 - Dam', 'Collect Result.35 - Dam', 'Collect Result.36 - Dam', 'Collect Result.37 - Dam', 'Collect Result.38 - Dam', 'Collect Result.42 - Dam', 'Collect Result.46 - Dam', 'Collect Result.50 - Dam', 'Collect Result.54 - Dam', 'Collect Result.55 - Dam', 'Collect Result.56 - Dam', 'Collect Result.57 - Dam', 'Collect Result.59 - Dam', 'Collect Result.60 - Dam', 'Collect Result.61 - Dam', 'Collect Result.62 - Dam', 'Collect Result.63 - Dam', 'Collect Result.64 - Dam', 'Collect Result.65 - Dam', 'Collect Result.67 - Dam', 'Collect Result.68 - Dam', 'Collect Result.69 - Dam', 'Insp. Seq No. - AutoClave', 'Collect Result - AutoClave', 'Collect Result.1 - AutoClave', 'Unit Time.1 - AutoClave', 'Unit Time.2 - AutoClave', 'Collect Result.3 - AutoClave', 'Unit Time.3 - AutoClave', 'Insp. Seq No. - Fill1', 'Collect Result.1 - Fill1', 'Collect Result.2 - Fill1', 'Collect Result.3 - Fill1', 'Collect Result.4 - Fill1', 'Collect Result.10 - Fill1', 'Collect Result.12 - Fill1', 'Collect Result.17 - Fill1', 'Collect Result.19 - Fill1', 'Collect Result.20 - Fill1', 'Collect Result.21 - Fill1', 'Collect Result.22 - Fill1', 'Collect Result.24 - Fill1', 'Collect Result.29 - Fill1', 'Insp. Seq No. - Fill2', 'Collect Result - Fill2', 'Collect Result.1 - Fill2', 'Collect Result.3 - Fill2', 'Collect Result.5 - Fill2', 'Collect Result.6 - Fill2', 'Collect Result.7 - Fill2', 'Collect Result.8 - Fill2', 'Collect Result.9 - Fill2', 'Collect Result.10 - Fill2', 'Collect Result.11 - Fill2', 'Collect Result.27 - Fill2', 'Collect Result.28 - Fill2', 'Collect Result.29 - Fill2', 'Collect Result.30 - Fill2', 'Collect Result.33 - Fill2', 'Collect Result.34 - Fill2', 'Collect Result.35 - Fill2', 'Collect Result.36 - Fill2', 'Collect Result.38 - Fill2', 'Collect Result.39 - Fill2', 'Wip Line - Dam', 'Process Desc. - Dam', 'Model.Suffix - Dam', 'Workorder - Dam', 'Insp Judge Code - Dam', 'Model.Suffix - AutoClave', 'Workorder - AutoClave', 'LOT ID - AutoClave', 'Insp Judge Code - AutoClave', 'Judge Value - AutoClave', 'Collect Result.4 - AutoClave', 'Judge Value.4 - AutoClave', 'Wip Line - Fill1', 'Process Desc. - Fill1', 'Workorder - Fill1', 'LOT ID - Fill1', 'Collect Result.7 - Fill1', 'Judge Value.7 - Fill1', 'Wip Line - Fill2', 'Model.Suffix - Fill2', 'Workorder - Fill2', 'Insp Judge Code - Fill2', 'Collect Result.17 - Fill2', 'Judge Value.17 - Fill2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.89372 | val_accuracy: 0.47943 |  0:00:01s\n",
      "epoch 1  | loss: 0.74403 | val_accuracy: 0.52482 |  0:00:01s\n",
      "epoch 2  | loss: 0.72491 | val_accuracy: 0.52624 |  0:00:01s\n",
      "epoch 3  | loss: 0.70643 | val_accuracy: 0.54681 |  0:00:01s\n",
      "epoch 4  | loss: 0.6954  | val_accuracy: 0.5383  |  0:00:01s\n",
      "epoch 5  | loss: 0.68811 | val_accuracy: 0.53262 |  0:00:01s\n",
      "epoch 6  | loss: 0.68865 | val_accuracy: 0.53759 |  0:00:01s\n",
      "epoch 7  | loss: 0.68379 | val_accuracy: 0.54823 |  0:00:02s\n",
      "epoch 8  | loss: 0.67879 | val_accuracy: 0.5539  |  0:00:02s\n",
      "epoch 9  | loss: 0.67939 | val_accuracy: 0.55248 |  0:00:02s\n",
      "epoch 10 | loss: 0.68374 | val_accuracy: 0.54326 |  0:00:02s\n",
      "epoch 11 | loss: 0.68045 | val_accuracy: 0.53901 |  0:00:02s\n",
      "epoch 12 | loss: 0.68199 | val_accuracy: 0.53901 |  0:00:02s\n",
      "epoch 13 | loss: 0.67709 | val_accuracy: 0.54823 |  0:00:02s\n",
      "epoch 14 | loss: 0.67654 | val_accuracy: 0.55106 |  0:00:02s\n",
      "epoch 15 | loss: 0.67587 | val_accuracy: 0.55532 |  0:00:02s\n",
      "epoch 16 | loss: 0.67449 | val_accuracy: 0.55177 |  0:00:02s\n",
      "epoch 17 | loss: 0.66933 | val_accuracy: 0.55816 |  0:00:03s\n",
      "epoch 18 | loss: 0.6716  | val_accuracy: 0.55461 |  0:00:03s\n",
      "epoch 19 | loss: 0.67309 | val_accuracy: 0.55532 |  0:00:03s\n",
      "epoch 20 | loss: 0.67129 | val_accuracy: 0.55745 |  0:00:03s\n",
      "epoch 21 | loss: 0.66919 | val_accuracy: 0.56738 |  0:00:03s\n",
      "epoch 22 | loss: 0.67021 | val_accuracy: 0.56312 |  0:00:03s\n",
      "epoch 23 | loss: 0.66704 | val_accuracy: 0.55603 |  0:00:03s\n",
      "epoch 24 | loss: 0.6676  | val_accuracy: 0.56525 |  0:00:03s\n",
      "epoch 25 | loss: 0.66811 | val_accuracy: 0.56241 |  0:00:03s\n",
      "epoch 26 | loss: 0.66795 | val_accuracy: 0.5617  |  0:00:04s\n",
      "epoch 27 | loss: 0.66644 | val_accuracy: 0.56738 |  0:00:04s\n",
      "epoch 28 | loss: 0.66857 | val_accuracy: 0.54255 |  0:00:04s\n",
      "epoch 29 | loss: 0.66819 | val_accuracy: 0.55745 |  0:00:04s\n",
      "epoch 30 | loss: 0.66729 | val_accuracy: 0.55248 |  0:00:04s\n",
      "epoch 31 | loss: 0.66486 | val_accuracy: 0.5539  |  0:00:04s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_accuracy = 0.56738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# 특성 선택을 위한 랜덤 포레스트 모델 정의\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Sequential Feature Selector 정의\n",
    "sfs = SFS(rf,\n",
    "          k_features='best',\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=3,  # 교차 검증 폴드 수를 줄여서 실행 시간 단축\n",
    "          n_jobs=-1)  # 병렬 처리 활성화\n",
    "\n",
    "# 특성 선택 수행\n",
    "sfs = sfs.fit(train_x, train_y)\n",
    "\n",
    "# 선택된 특성 목록\n",
    "selected_features = [features[i] for i in sfs.k_feature_idx_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# 선택된 특성을 사용하여 데이터 재구성\n",
    "train_x_selected = df_train[selected_features].values\n",
    "test_x_selected = df_val[selected_features].values\n",
    "\n",
    "# TabNet 모델 정의 및 학습\n",
    "tabnet_model = TabNetClassifier(seed=RANDOM_STATE)\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_train=train_x_selected,\n",
    "    y_train=train_y,\n",
    "    eval_set=[(test_x_selected, test_y)],\n",
    "    eval_name=['val'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1410, 3290]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 평가 결과 출력\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNormal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAbNormal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion_matrix(test_y, preds_val))\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2604\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2470\u001b[0m     {\n\u001b[0;32m   2471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2495\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2496\u001b[0m ):\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2498\u001b[0m \n\u001b[0;32m   2499\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2601\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2604\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2607\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1410, 3290]"
     ]
    }
   ],
   "source": [
    "# 검증 데이터 예측\n",
    "preds_val = tabnet_model.predict(train_x_selected)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_y, preds_val, target_names=['Normal', 'AbNormal']))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_y, preds_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 매핑\n",
    "label_mapping = {0: 'Normal', 1: 'AbNormal'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 예측\n",
    "test_pred = tabnet_model.predict(df_test_x)\n",
    "\n",
    "# 숫자 레이블을 문자열로 변환\n",
    "test_pred_labels = [label_mapping[pred] for pred in test_pred]\n",
    "\n",
    "# 제출 파일 준비\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = test_pred_labels\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
