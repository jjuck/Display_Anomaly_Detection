{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\train_mod.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m ROOT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m RANDOM_STATE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m110\u001b[39m\n\u001b[1;32m---> 17\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_mod.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\train_mod.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import shap\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE\n",
    "\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def featuregen(train_data):\n",
    "    axis = ['X', 'Y', 'Z']\n",
    "    process = ['Dam', 'Fill1', 'Fill2']\n",
    "\n",
    "    for ax in axis:\n",
    "        for proc in process:\n",
    "            stage1_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage1) Collect Result_{proc}'\n",
    "            stage2_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage2) Collect Result_{proc}'\n",
    "            stage3_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage3) Collect Result_{proc}'\n",
    "            new_col_1_3 = f'Head_DIFF_{ax}_Stage1&3_{proc}'\n",
    "            new_col_max_min = f'Head_MinMax_{ax}_{proc}'\n",
    "            \n",
    "        \n",
    "            train_data[new_col_1_3] = (train_data[stage1_col] - train_data[stage3_col]).abs()\n",
    "            train_data[new_col_max_min] = train_data[[stage1_col, stage2_col, stage3_col]].max(axis=1) - train_data[[stage1_col, stage2_col, stage3_col]].min(axis=1)\n",
    "\n",
    "    return train_data\n",
    "                                                                                                            \n",
    "def generate_stage_averages(df):\n",
    "    stages = ['Stage1', 'Stage2', 'Stage3']\n",
    "    \n",
    "    for stage in stages:\n",
    "        stage_columns = [col for col in df.columns if stage in col and ('Circle' in col or 'Line' in col)]\n",
    "        df[f'{stage}_Average'] = df[stage_columns].mean(axis=1)\n",
    "    \n",
    "    return df\n",
    "                                                                                                            \n",
    "def generating_features(df):\n",
    "    df['Thickness_Diff_1_2'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 2 Collect Result_Dam']\n",
    "    df['Thickness_Diff_2_3'] = df['THICKNESS 2 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "    df['Thickness_Std'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].std(axis=1)\n",
    "    df['Thickness_Max_Min_Diff'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].max(axis=1) - df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].min(axis=1)\n",
    "    df['Temperature_Change_Rate'] = df['Chamber Temp. Collect Result_AutoClave'] * df['Chamber Temp. Unit Time_AutoClave']\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_volume_to_speed_ratio(df):\n",
    "    df['Volume_to_Speed_Ratio_Stage1'] = df['Dispense Volume(Stage1) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_to_Speed_Ratio_Stage2'] = df['Dispense Volume(Stage2) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_to_Speed_Ratio_Stage3'] = df['Dispense Volume(Stage3) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_Sum_Fill1'] = df['Dispense Volume(Stage1) Collect Result_Fill1'] + df['Dispense Volume(Stage2) Collect Result_Fill1'] + df['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_pressure_change_rate(df):\n",
    "    df['Pressure_Change_Rate_1st'] = df['1st Pressure Collect Result_AutoClave'] * df['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "    df['Pressure_Change_Rate_2nd'] = df['2nd Pressure Collect Result_AutoClave'] * df['2nd Pressure Unit Time_AutoClave']\n",
    "    df['Pressure_Change_Rate_3rd'] = df['3rd Pressure Collect Result_AutoClave'] * df['3rd Pressure Unit Time_AutoClave']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_volume_to_time_ratio(df):\n",
    "    df['Volume_to_Time_Ratio_Stage1'] = df['Dispense Volume(Stage1) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "    df['Volume_to_Time_Ratio_Stage2'] = df['Dispense Volume(Stage2) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "    df['Volume_to_Time_Ratio_Stage3'] = df['Dispense Volume(Stage3) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = featuregen(train_data)\n",
    "train_data = generating_features(train_data)\n",
    "train_data = generate_volume_to_speed_ratio(train_data)\n",
    "train_data = generate_pressure_change_rate(train_data)\n",
    "train_data = generate_volume_to_time_ratio(train_data)\n",
    "train_data = generate_stage_averages(train_data)\n",
    "\n",
    "\n",
    "train_data = train_data[[\n",
    "    \n",
    "                         'Head_DIFF_X_Stage1&3_Dam',\n",
    "                         'Head_DIFF_X_Stage1&3_Fill1',\n",
    "                         'Head_DIFF_X_Stage1&3_Fill2',\n",
    "                         \n",
    "                         'Head_MinMax_Y_Dam',\n",
    "                         'Head_MinMax_Y_Fill1',\n",
    "                         'Head_MinMax_Y_Fill2',\n",
    "                         \n",
    "                         'Stage1_Average',\n",
    "                         'Stage2_Average',\n",
    "                         'Stage3_Average',\n",
    "\n",
    "                         'Thickness_Max_Min_Diff',\n",
    "#                          'Thickness_Std',\n",
    "                         \n",
    "                         'Temperature_Change_Rate',\n",
    "                         \n",
    "                         'Pressure_Change_Rate_1st',\n",
    "                         'Pressure_Change_Rate_2nd',\n",
    "                         'Pressure_Change_Rate_3rd',\n",
    "                         \n",
    "                         'Volume_to_Time_Ratio_Stage1',\n",
    "                         'Volume_to_Time_Ratio_Stage2',\n",
    "                         'Volume_to_Time_Ratio_Stage3',\n",
    "    \n",
    "                         'Volume_Sum_Fill1',\n",
    "                         'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    "    \n",
    "                         'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "\n",
    "X = train_data.drop(columns=['target'])\n",
    "y = train_data['target']\n",
    "\n",
    "smote = SMOTEENN(sampling_strategy= {0:10000}, random_state=RANDOM_STATE)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "train_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='target')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Normal: 32144, AbNormal: 3756\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "# scaler = RobustScaler()\n",
    "scaler = StandardScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "\n",
    "# Undersample the data\n",
    "normal_ratio = 1.0  # 1:1 ratio\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "df_normal = df_normal.sample(n=int(num_abnormal * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "train_data = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "train_x = train_data.drop(columns=['target'])\n",
    "train_y = train_data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9536\n",
      "Recall: 0.9759\n",
      "Accuracy: 0.9528\n",
      "Precision: 0.9322\n",
      "Pressure_Change_Rate_3rd                          1.063457\n",
      "Volume_Sum_Fill1                                  0.649318\n",
      "Pressure_Change_Rate_2nd                          0.447355\n",
      "Pressure_Change_Rate_1st                          0.392265\n",
      "Temperature_Change_Rate                           0.387186\n",
      "Head_DIFF_X_Stage1&3_Dam                          0.337587\n",
      "Head_MinMax_Y_Fill1                               0.299558\n",
      "Volume_to_Time_Ratio_Stage2                       0.250895\n",
      "Volume_to_Time_Ratio_Stage1                       0.233071\n",
      "Stage2_Average                                    0.202845\n",
      "Volume_to_Time_Ratio_Stage3                       0.166884\n",
      "Head_MinMax_Y_Dam                                 0.151236\n",
      "Head_MinMax_Y_Fill2                               0.106507\n",
      "Head_DIFF_X_Stage1&3_Fill1                        0.100302\n",
      "Head_DIFF_X_Stage1&3_Fill2                        0.069944\n",
      "Thickness_Max_Min_Diff                            0.059451\n",
      "Stage1_Average                                    0.051646\n",
      "Stage3_Average                                    0.044458\n",
      "DISCHARGED SPEED OF RESIN Collect Result_Fill1    0.034029\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination (RFE) for backward feature selection\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=500,\n",
    "    l2_leaf_reg=3,\n",
    "    learning_rate=0.1,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# Train final model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "#SHAP\n",
    "# SHAP value caculation\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Features importance\n",
    "df_shap = pd.DataFrame(shap_values, columns=X_test.columns)\n",
    "shap_importance = df_shap.abs().mean().sort_values(ascending=False)\n",
    "print(shap_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of abnormal is: 0.04101146247335983\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "test_data = featuregen(test_data)\n",
    "test_data = generating_features(test_data) \n",
    "test_data = generate_volume_to_speed_ratio(test_data)\n",
    "test_data = generate_pressure_change_rate(test_data)\n",
    "test_data = generate_volume_to_time_ratio(test_data)\n",
    "test_data = generate_stage_averages(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_rfe = test_data[X_train.columns]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_rfe)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV,  GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# # 타겟 변수와 피처 변수 분리\n",
    "# y = train_data['target']\n",
    "# X = train_data.drop(columns=['target'])\n",
    "\n",
    "# # 학습 데이터와 테스트 데이터로 분리\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=RANDOM_STATE)\n",
    "\n",
    "# # CatBoostClassifier 모델 설정 (GPU 사용)\n",
    "# model = CatBoostClassifier(\n",
    "#     task_type='CPU',  # GPU 사용 설정\n",
    "#     verbose=0  # 출력 로그를 줄이기 위해 설정\n",
    "# )\n",
    "\n",
    "# # 하이퍼파라미터 그리드 정의\n",
    "# param_grid = {\n",
    "#     'depth': [4, 5, 6],\n",
    "#     'iterations': [300, 400, 500],\n",
    "#     'l2_leaf_reg': [3, 5, 7],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2]\n",
    "# }\n",
    "\n",
    "# # GridSearchCV 설정\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='accuracy',\n",
    "#     cv=3,  # 교차 검증 폴드 수\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1  # 병렬 처리\n",
    "# )\n",
    "\n",
    "# # Grid Search 학습\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # 최적의 하이퍼파라미터와 모델\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# # 테스트 데이터에 대한 예측\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # 성능 지표 계산\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1-score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
