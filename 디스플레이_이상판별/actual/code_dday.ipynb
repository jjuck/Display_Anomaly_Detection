{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.feature_selection import RFE\n",
    "import shap\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, BorderlineSMOTE, SVMSMOTE, ADASYN, KMeansSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, CondensedNearestNeighbour, OneSidedSelection, InstanceHardnessThreshold, AllKNN, RepeatedEditedNearestNeighbours, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train_mod.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat2num(X):\n",
    "    non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "    # print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "    encoded_columns = {}\n",
    "\n",
    "    for column in non_numeric_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoded_columns[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_columns, index=X.index)\n",
    "\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "    X = pd.concat([X, encoded_df], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def featuregen(train_data):\n",
    "    axis = ['X', 'Y', 'Z']\n",
    "    process = ['Dam', 'Fill1', 'Fill2']\n",
    "\n",
    "    for ax in axis:\n",
    "        for proc in process:\n",
    "            stage1_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage1) Collect Result_{proc}'\n",
    "            stage2_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage2) Collect Result_{proc}'\n",
    "            stage3_col = f'HEAD NORMAL COORDINATE {ax} AXIS(Stage3) Collect Result_{proc}'\n",
    "            new_col_1_3 = f'Head_DIFF_{ax}_Stage1&3_{proc}'\n",
    "            new_col_max_min = f'Head_MinMax_{ax}_{proc}'\n",
    "            \n",
    "        \n",
    "            train_data[new_col_1_3] = (train_data[stage1_col] - train_data[stage3_col]).abs()\n",
    "            train_data[new_col_max_min] = train_data[[stage1_col, stage2_col, stage3_col]].max(axis=1) - train_data[[stage1_col, stage2_col, stage3_col]].min(axis=1)\n",
    "\n",
    "    return train_data\n",
    "                                                                                                            \n",
    "def generate_stage_averages(df):\n",
    "    stages = ['Stage1', 'Stage2', 'Stage3']\n",
    "    \n",
    "    for stage in stages:\n",
    "        stage_columns = [col for col in df.columns if stage in col and ('Circle' in col or 'Line' in col)]\n",
    "        df[f'{stage}_Average'] = df[stage_columns].mean(axis=1)\n",
    "    \n",
    "    return df\n",
    "                                                                                                            \n",
    "def generating_features(df):\n",
    " \n",
    "    # Thickness difference\n",
    "    df['Thickness_Diff_1_2'] = df['THICKNESS 1 Collect Result_Dam'] - df['THICKNESS 2 Collect Result_Dam']\n",
    "    df['Thickness_Diff_2_3'] = df['THICKNESS 2 Collect Result_Dam'] - df['THICKNESS 3 Collect Result_Dam']\n",
    "    df['Thickness_Std'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].std(axis=1)\n",
    "    df['Thickness_Max_Min_Diff'] = df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].max(axis=1) - df[['THICKNESS 1 Collect Result_Dam', 'THICKNESS 2 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam']].min(axis=1)\n",
    "    df['Temperature_Change_Rate'] = df['Chamber Temp. Collect Result_AutoClave'] * df['Chamber Temp. Unit Time_AutoClave']\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_volume_to_speed_ratio(df):\n",
    "    df['Volume_to_Speed_Ratio_Stage1'] = df['Dispense Volume(Stage1) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_to_Speed_Ratio_Stage2'] = df['Dispense Volume(Stage2) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_to_Speed_Ratio_Stage3'] = df['Dispense Volume(Stage3) Collect Result_Fill1'] / df['DISCHARGED SPEED OF RESIN Collect Result_Fill1']\n",
    "    df['Volume_Sum_Fill1'] = df['Dispense Volume(Stage1) Collect Result_Fill1'] + df['Dispense Volume(Stage2) Collect Result_Fill1'] + df['Dispense Volume(Stage3) Collect Result_Fill1']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_pressure_change_rate(df):\n",
    "    df['Pressure_Change_Rate_1st'] = df['1st Pressure Collect Result_AutoClave'] * df['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "    df['Pressure_Change_Rate_2nd'] = df['2nd Pressure Collect Result_AutoClave'] * df['2nd Pressure Unit Time_AutoClave']\n",
    "    df['Pressure_Change_Rate_3rd'] = df['3rd Pressure Collect Result_AutoClave'] * df['3rd Pressure Unit Time_AutoClave']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_volume_to_time_ratio(df):\n",
    "    df['Volume_to_Time_Ratio_Stage1'] = df['Dispense Volume(Stage1) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "    df['Volume_to_Time_Ratio_Stage2'] = df['Dispense Volume(Stage2) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "    df['Volume_to_Time_Ratio_Stage3'] = df['Dispense Volume(Stage3) Collect Result_Dam'] / df['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train_data = cat2num(train_data)\n",
    "train_data = featuregen(train_data)\n",
    "train_data = generating_features(train_data)\n",
    "train_data = generate_volume_to_speed_ratio(train_data)\n",
    "train_data = generate_pressure_change_rate(train_data)\n",
    "train_data = generate_volume_to_time_ratio(train_data)\n",
    "train_data = generate_stage_averages(train_data)\n",
    "\n",
    "                                                                                                            \n",
    "\n",
    "\n",
    "\n",
    "train_data = train_data[[\n",
    "    \n",
    "                         'Head_DIFF_X_Stage1&3_Dam',\n",
    "                         'Head_DIFF_X_Stage1&3_Fill1',\n",
    "                         'Head_DIFF_X_Stage1&3_Fill2',\n",
    "                         \n",
    "                         'Head_MinMax_Y_Dam',\n",
    "                         'Head_MinMax_Y_Fill1',\n",
    "                         'Head_MinMax_Y_Fill2',\n",
    "                         \n",
    "                         'Stage1_Average',\n",
    "                         'Stage2_Average',\n",
    "                         'Stage3_Average',\n",
    "\n",
    "                         'Thickness_Max_Min_Diff',\n",
    "#                          'Thickness_Std',\n",
    "                         \n",
    "                         'Temperature_Change_Rate',\n",
    "                         \n",
    "                         'Pressure_Change_Rate_1st',\n",
    "                         'Pressure_Change_Rate_2nd',\n",
    "                         'Pressure_Change_Rate_3rd',\n",
    "                         \n",
    "                         'Volume_to_Time_Ratio_Stage1',\n",
    "                         'Volume_to_Time_Ratio_Stage2',\n",
    "                         'Volume_to_Time_Ratio_Stage3',\n",
    "    \n",
    "                         'Volume_Sum_Fill1',\n",
    "                         'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    "    \n",
    "#                          'Receip No Collect Result_Dam', \n",
    "#                          'Receip No Collect Result_Fill1',  \n",
    "#     'Production Qty Collect Result_Fill1',\n",
    "#     'Production Qty Collect Result_Fill2',\n",
    "#     'Production Qty Collect Result_Dam',\n",
    "    \n",
    "    \n",
    "                         'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Normal: 10000, AbNormal: 10000\n"
     ]
    }
   ],
   "source": [
    "X = train_data.drop(columns=['target'])\n",
    "y = train_data['target']\n",
    "\n",
    "# 오버샘플링\n",
    "# oversampler = BorderlineSMOTE(sampling_strategy={0 : 10000}, random_state=RANDOM_STATE)\n",
    "# borderline_smote = BorderlineSMOTE(sampling_strategy=\"auto\", random_state=RANDOM_STATE)\n",
    "oversampler = SMOTEENN(sampling_strategy= {0 : 10000}, random_state=RANDOM_STATE)\n",
    "X, y = oversampler.fit_resample(X, y)\n",
    "\n",
    "# 언더샘플링\n",
    "# undersampler = RandomUnderSampler(sampling_strategy={1: 10000}, random_state=RANDOM_STATE)\n",
    "# undersampler = RandomUnderSampler(sampling_strategy=\"auto\", random_state=RANDOM_STATE)\n",
    "# undersampler = NearMiss(sampling_strategy=\"auto\")\n",
    "\n",
    "# undersampler = CondensedNearestNeighbour(sampling_strategy='auto')\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# undersampler = TomekLinks(sampling_strategy='auto')\n",
    "# undersampler = OneSidedSelection(sampling_strategy='auto', random_state=RANDOM_STATE)\n",
    "# undersampler = AllKNN(sampling_strategy='auto')\n",
    "# undersampler = RepeatedEditedNearestNeighbours(sampling_strategy='auto')\n",
    "# undersampler = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# X, y = undersampler.fit_resample(X, y)\n",
    "\n",
    "# undersampler = NearMiss(sampling_strategy=\"auto\")\n",
    "# clf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "# undersampler = InstanceHardnessThreshold(estimator=clf, sampling_strategy='auto', random_state=RANDOM_STATE)\n",
    "undersampler = RandomUnderSampler(sampling_strategy=\"auto\", random_state=RANDOM_STATE)\n",
    "X, y = undersampler.fit_resample(X, y)\n",
    "\n",
    "# 데이터프레임으로 다시 결합\n",
    "train_data = pd.concat([pd.DataFrame(X, columns=X.columns), pd.Series(y, name='target')], axis=1)\n",
    "\n",
    "df_normal = train_data[train_data[\"target\"] == 1]\n",
    "df_abnormal = train_data[train_data[\"target\"] == 0]\n",
    "num_normal = len(df_normal)\n",
    "num_abnormal = len(df_abnormal)\n",
    "print(f\"Total: Normal: {num_normal}, AbNormal: {num_abnormal}\")\n",
    "\n",
    "# 스케일링 (정규화 또는 표준화)\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = [col for col in train_data.columns if col != 'target']\n",
    "train_data[columns_to_scale] = scaler.fit_transform(train_data[columns_to_scale])\n",
    "\n",
    "# 데이터를 다시 피처와 타겟으로 분리\n",
    "train_x = train_data.drop(columns=['target'])\n",
    "train_y = train_data['target']\n",
    "\n",
    "# 학습 및 테스트 데이터로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.22,\n",
    "    random_state=RANDOM_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8130\n",
      "Recall: 0.8668\n",
      "Accuracy: 0.8020\n",
      "Precision: 0.7655\n",
      "Pressure_Change_Rate_3rd                          0.329578\n",
      "Volume_Sum_Fill1                                  0.157052\n",
      "Pressure_Change_Rate_2nd                          0.117980\n",
      "Volume_to_Time_Ratio_Stage1                       0.117130\n",
      "Stage2_Average                                    0.093866\n",
      "Head_DIFF_X_Stage1&3_Dam                          0.092403\n",
      "Head_MinMax_Y_Fill1                               0.090238\n",
      "Head_MinMax_Y_Dam                                 0.083981\n",
      "Volume_to_Time_Ratio_Stage3                       0.069989\n",
      "Stage3_Average                                    0.066497\n",
      "Volume_to_Time_Ratio_Stage2                       0.064517\n",
      "Head_MinMax_Y_Fill2                               0.055372\n",
      "Stage1_Average                                    0.050399\n",
      "Temperature_Change_Rate                           0.033667\n",
      "Head_DIFF_X_Stage1&3_Fill1                        0.029103\n",
      "Head_DIFF_X_Stage1&3_Fill2                        0.024357\n",
      "Pressure_Change_Rate_1st                          0.022547\n",
      "Thickness_Max_Min_Diff                            0.019572\n",
      "DISCHARGED SPEED OF RESIN Collect Result_Fill1    0.006527\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination (RFE) for backward feature selection\n",
    "model = cb.CatBoostClassifier(\n",
    "    depth=6,\n",
    "    iterations=500,\n",
    "    l2_leaf_reg=3,\n",
    "    learning_rate=0.01,\n",
    "    verbose=0  # No output during training\n",
    ")\n",
    "\n",
    "# catboost_model = cb.CatBoostClassifier(\n",
    "#     depth=6,\n",
    "#     iterations=500,\n",
    "#     l2_leaf_reg=3,\n",
    "#     learning_rate=0.05,\n",
    "#     verbose=0  # No output during training\n",
    "# )\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     max_depth = 4,\n",
    "#     min_child_weight = 1,\n",
    "#     gamma = 0.3,\n",
    "#     colsample_bytree = 1.0,\n",
    "#     n_etimators = 50,\n",
    "#     reg_alpha = 0.01,\n",
    "#     reg_lambda = 1.5,\n",
    "#     sub_sample = 1.0,\n",
    "#     learning_rate = 0.005, RANDOM_STATE = 110)\n",
    "\n",
    "# model = StackingClassifier(\n",
    "#     estimators=[('catboost', catboost_model), ('xgb', xgb_model)],\n",
    "#     final_estimator=LogisticRegression()\n",
    "# )\n",
    "\n",
    "# Train final model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "#SHAP\n",
    "# SHAP value caculation\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Features importance\n",
    "df_shap = pd.DataFrame(shap_values, columns=X_test.columns)\n",
    "shap_importance = df_shap.abs().mean().sort_values(ascending=False)\n",
    "print(shap_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of abnormal is: 0.13132884050457924\n"
     ]
    }
   ],
   "source": [
    "# Process test data\n",
    "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test_mod.csv\"))\n",
    "test_data = cat2num(test_data)\n",
    "test_data = featuregen(test_data)\n",
    "test_data = generating_features(test_data) \n",
    "test_data = generate_volume_to_speed_ratio(test_data)\n",
    "test_data = generate_pressure_change_rate(test_data)\n",
    "test_data = generate_volume_to_time_ratio(test_data)\n",
    "test_data = generate_stage_averages(test_data)\n",
    "\n",
    "# Scale the test data\n",
    "test_data[columns_to_scale] = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "# Select the same features as the training data\n",
    "test_x_rfe = test_data[X_train.columns]\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(test_x_rfe)\n",
    "\n",
    "# Prepare submission\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = y_pred\n",
    "df_sub['target'] = df_sub['target'].map({1: 'Normal', 0: 'AbNormal'})\n",
    "\n",
    "# Calculate the ratio of abnormal cases\n",
    "counts = df_sub['target'].value_counts()\n",
    "ratio = counts['AbNormal'] / (counts['AbNormal'] + counts['Normal'])\n",
    "print(\"The ratio of abnormal is:\", ratio)\n",
    "\n",
    "# Save the submission file\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.16615760537568722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.model_selection import train_test_split, RandomizedSearchCV,  GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# # 타겟 변수와 피처 변수 분리\n",
    "# y = train_data['target']\n",
    "# X = train_data.drop(columns=['target'])\n",
    "\n",
    "# # 학습 데이터와 테스트 데이터로 분리\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=RANDOM_STATE)\n",
    "\n",
    "# # CatBoostClassifier 모델 설정 (GPU 사용)\n",
    "# model = CatBoostClassifier(\n",
    "#     task_type='CPU',  # GPU 사용 설정\n",
    "#     verbose=0  # 출력 로그를 줄이기 위해 설정\n",
    "# )\n",
    "\n",
    "# # 하이퍼파라미터 그리드 정의\n",
    "# param_grid = {\n",
    "#     'depth': [4, 5, 6],\n",
    "#     'iterations': [300, 400, 500],\n",
    "#     'l2_leaf_reg': [3, 5, 7],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2]\n",
    "# }\n",
    "\n",
    "# # GridSearchCV 설정\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=model,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='accuracy',\n",
    "#     cv=3,  # 교차 검증 폴드 수\n",
    "#     verbose=1,\n",
    "#     n_jobs=-1  # 병렬 처리\n",
    "# )\n",
    "\n",
    "# # Grid Search 학습\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # 최적의 하이퍼파라미터와 모델\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# # 테스트 데이터에 대한 예측\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# # 성능 지표 계산\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, average='weighted')\n",
    "# recall = recall_score(y_test, y_pred, average='weighted')\n",
    "# f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# print(f'Accuracy: {accuracy:.4f}')\n",
    "# print(f'Precision: {precision:.4f}')\n",
    "# print(f'Recall: {recall:.4f}')\n",
    "# print(f'F1-score: {f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
